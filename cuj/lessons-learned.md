# Post-Mortem Reflection: Insights from the CUJ Framework

Before conducting the Critical User Journey (CUJ) analysis, our team largely equated “done” with “functional.” As long as the core features worked—randomizing teams, displaying information, and running timers—we felt the product was complete enough to move on. The CUJ framework challenged this assumption by forcing us to slow down and examine how the tool is actually used in a real classroom context, step by step, under time pressure.

One of the most impactful shifts in perspective came from explicitly **quantifying user friction**. Breaking the workflow into concrete actions and assigning rough time estimates made us realize how even small inefficiencies accumulate across many repetitions. For example, starting a timer takes only a second, but when repeated for every team, across presentation and Q&A phases, any confusion or missed step compounds quickly. What initially felt like a minor inconvenience became clearly visible as recurring cognitive load.

The CUJ process also highlighted the difference between a feature existing and a feature being usable. Our timers worked as intended, but the analysis revealed that transitioning between presentation and Q&A required the instructor to remember the workflow rather than being guided by the interface. From a development perspective, the feature was “done.” From a user perspective, it was fragile—easy to forget, easy to misuse, and easy to misinterpret under real classroom conditions.

Another important insight came from identifying context switches. We initially assumed that the instructor would remain focused on the app during presentations. The CUJ made it clear that this assumption was unrealistic. In practice, instructors frequently switch attention to rubrics, slides, student questions, or technical issues. When they return to the app, the interface needs to help them quickly re-orient. The lack of session-state feedback (for example, how many teams have already presented) stood out as a usability gap that we would not have noticed without explicitly mapping the journey.

Quantifying friction also helped us become more disciplined about scope. Instead of immediately adding features to “fix” every problem, we learned to distinguish between critical usability issues and acceptable limitations of an MVP. The CUJ framework encouraged us to document pain points honestly rather than prematurely solving them, reinforcing that learning and iteration—not polish—were the goals of this assignment.

Overall, the CUJ framework reframed our understanding of progress. A product can be technically complete while still placing unnecessary mental burden on the user. By measuring friction, we shifted from asking “Does this work?” to asking “Does this help the user do their job under realistic conditions?” This distinction between “done” and “usable” is a perspective we will carry forward into future projects, especially when building tools intended for real-world, time-constrained environments.